{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R04XrjDmvn8A"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqrxIajYbWZg"
      },
      "source": [
        "## 1. Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bJqatHVvMQD",
        "outputId": "951f5549-e73c-4806-d2ba-a9189136781a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at gdrive; to attempt to forcibly remount, call drive.mount(\"gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import drive\n",
        "import cv2 as cv\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "drive.mount('gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yac6pdlvaxu2"
      },
      "source": [
        "## 2. Importing and pre-processing data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0adM57KWdS",
        "outputId": "4e5a0bcb-1419-4349-ac60-4306fb62efb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Using 1600 files for training.\n",
            "Found 2000 files belonging to 2 classes.\n",
            "Using 400 files for validation.\n"
          ]
        }
      ],
      "source": [
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "  'gdrive/MyDrive/2A/MODAL/dataset4', labels='inferred', seed = 1, shuffle = True, batch_size = 64, image_size=(640, 480),color_mode='grayscale',validation_split = 0.2, subset='training')\n",
        "\n",
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "  'gdrive/MyDrive/2A/MODAL/dataset4', labels='inferred', seed = 1, shuffle = True, batch_size = 64, image_size=(640, 480),color_mode='grayscale',validation_split = 0.2, subset='validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJO0h6MQa-yF"
      },
      "source": [
        "## 3. Developing the architecture of the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JNHoz2TPIB7"
      },
      "outputs": [],
      "source": [
        "#defining our model: inputs and outputs\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(640, 480, 1))\n",
        "\n",
        "# Center-crop images to 150x150\n",
        "x = layers.CenterCrop(height=400, width=400)(inputs)\n",
        "# Rescale images to [0, 1]\n",
        "x = layers.Rescaling(scale=1.0 / 255)(x)\n",
        "\n",
        "# Apply some convolution and pooling layers\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
        "\n",
        "# Apply global average pooling to get flat feature vectors\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a dense classifier on top\n",
        "num_classes = 2\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oW7DvPRabIm",
        "outputId": "37136ee7-2fc3-462f-8f1b-5a930be28982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 640, 480, 1)]     0         \n",
            "                                                                 \n",
            " center_crop_1 (CenterCrop)  (None, 400, 400, 1)       0         \n",
            "                                                                 \n",
            " rescaling_1 (Rescaling)     (None, 400, 400, 1)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 398, 398, 32)      320       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 132, 132, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 130, 130, 32)      9248      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 43, 43, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 41, 41, 32)        9248      \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 32)               0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,882\n",
            "Trainable params: 18,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmAcjGumbEI1"
      },
      "source": [
        "## 3. Training the model on dataset 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bxRwKKQatol",
        "outputId": "b48e0dfa-351b-4cc5-8523-c9815178c931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25/25 [==============================] - 16s 442ms/step - loss: 0.2636 - acc: 0.9069 - val_loss: 0.2759 - val_acc: 0.8350\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 14s 436ms/step - loss: 0.2377 - acc: 0.9275 - val_loss: 0.2183 - val_acc: 0.9625\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 13s 431ms/step - loss: 0.2107 - acc: 0.9475 - val_loss: 0.1973 - val_acc: 0.9750\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 14s 436ms/step - loss: 0.1886 - acc: 0.9712 - val_loss: 0.1901 - val_acc: 0.9275\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 14s 436ms/step - loss: 0.1831 - acc: 0.9563 - val_loss: 0.1939 - val_acc: 0.9075\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 14s 436ms/step - loss: 0.1695 - acc: 0.9581 - val_loss: 0.1514 - val_acc: 0.9850\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 14s 437ms/step - loss: 0.1483 - acc: 0.9806 - val_loss: 0.1331 - val_acc: 0.9900\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 14s 437ms/step - loss: 0.1395 - acc: 0.9737 - val_loss: 0.1244 - val_acc: 0.9750\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 14s 434ms/step - loss: 0.1331 - acc: 0.9794 - val_loss: 0.1221 - val_acc: 0.9825\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 14s 434ms/step - loss: 0.1146 - acc: 0.9875 - val_loss: 0.1009 - val_acc: 0.9975\n",
            "INFO:tensorflow:Assets written to: gdrive/MyDrive/2A/MODAL/trial8/assets\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
        "\n",
        "history = model.fit(train_ds, epochs=10, validation_data = val_ds)\n",
        "\n",
        "#callbacks = tf.keras.callbacks.EarlyStopping(verbose=1, patience=3)\n",
        "\n",
        "model.save('gdrive/MyDrive/2A/MODAL/trial8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "G0PDcVukZQ6v",
        "outputId": "1ac36328-4161-4839-b439-69701f92bbff"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "with open('history3.txt', 'w') as f:\n",
        "    f.write(\"acc\")\n",
        "    f.write('\\n')\n",
        "    f.write(str(history.history['acc']))    \n",
        "    f.write('\\n')\n",
        "    f.write(\"val_acc\")\n",
        "    f.write('\\n')\n",
        "    f.write(str(history.history['val_acc']))\n",
        "    f.write('\\n')\n",
        "    f.write(\"val_loss\")\n",
        "    f.write('\\n')\n",
        "    f.write(str(history.history['val_loss']))\n",
        "    f.write('\\n')\n",
        "    f.write(\"loss\")\n",
        "    f.write('\\n')  \n",
        "    f.write(str(history.history['loss']))\n",
        "    f.write('\\n')      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UqJ4XqqmhD4"
      },
      "source": [
        "## 5. Evaluation of model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9sixskVVObO",
        "outputId": "ebd11cab-9dba-4e23-9a3c-287a837184d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400 files belonging to 2 classes.\n",
            "400/400 [==============================] - 4s 9ms/step - loss: 0.1026 - acc: 0.9950\n",
            "loss: 0.10\n",
            "acc: 1.00\n"
          ]
        }
      ],
      "source": [
        "test_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "  'gdrive/MyDrive/2A/MODAL/testset4', labels='inferred', seed = 1, batch_size = 1, shuffle = True, image_size=(640, 480),color_mode='grayscale')\n",
        "\n",
        "loss, acc = model.evaluate(test_ds)  # returns loss and metrics\n",
        "print(\"loss: %.2f\" % loss)\n",
        "print(\"acc: %.2f\" % acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbdjl2bnVW_B"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "predictions = []\n",
        "labels =  []\n",
        "for x, y in test_ds:\n",
        "  y_pred = model.predict(x)\n",
        "  if y_pred[0][0]>0.5:\n",
        "      predictions.append(0)\n",
        "  else:\n",
        "      predictions.append(1)\n",
        "\n",
        "  labels.append(y.numpy()[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3VqaxPncnpa",
        "outputId": "d743efd8-e28e-4b59-c14e-3f31f9e868af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[200   0]\n",
            " [  2 198]]\n"
          ]
        }
      ],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(labels=np.array(labels), predictions=np.array(predictions)).numpy()\n",
        "print(confusion_mtx)\n",
        "\n",
        "#The matrix columns represent the prediction labels and the rows represent the real labels. \n",
        "#[0][0] -> true positive (circle classified as circle)\n",
        "#[0][1] -> false negative (circle classified as square)\n",
        "#[1][0] -> false positive (square classified as circle)\n",
        "#[1][1] -> true negative (square classified as square)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rhClbYN9eyL"
      },
      "source": [
        "# Raspberry dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeEULEO-9eE7",
        "outputId": "84b4a76e-a659-437b-9555-994b2f945815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 files belonging to 2 classes.\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.1488 - acc: 0.7500\n",
            "loss: 1.15\n",
            "acc: 0.75\n"
          ]
        }
      ],
      "source": [
        "raspberry_ds = keras.preprocessing.image_dataset_from_directory('gdrive/MyDrive/2A/MODAL/raspberrytest', labels='inferred', batch_size = 1, image_size=(640, 480),color_mode='grayscale')\n",
        "\n",
        "loss, acc = model.evaluate(raspberry_ds)  # returns loss and metrics\n",
        "print(\"loss: %.2f\" % loss)\n",
        "print(\"acc: %.2f\" % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvVCYS-Lx8T3",
        "outputId": "e388fdbe-f36a-492a-c989-d0e48be25bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i = 0\n",
            "y = 0\n",
            "i = 1\n",
            "y = 0\n",
            "i = 4\n",
            "y = 0\n",
            "i = 7\n",
            "y = 0\n",
            "[[4 2]\n",
            " [0 4]]\n"
          ]
        }
      ],
      "source": [
        "rb_predictions = []\n",
        "rb_labels =  []\n",
        "i=0\n",
        "for x, y in raspberry_ds:\n",
        "  rb_y_pred = model.predict(x)\n",
        "  if rb_y_pred[0][0]>0.5:\n",
        "      rb_predictions.append(0)\n",
        "      print(\"i = \" + str(i))\n",
        "      print(\"y = \" + str(y.numpy()[0]))\n",
        "  else:\n",
        "      rb_predictions.append(1)\n",
        "  i+=1\n",
        "\n",
        "  rb_labels.append(y.numpy()[0])\n",
        "\n",
        "rb_confusion_mtx = tf.math.confusion_matrix(labels=np.array(rb_labels), predictions=np.array(rb_predictions)).numpy()\n",
        "print(rb_confusion_mtx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_yULZ-cbR2E"
      },
      "source": [
        "## Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rKYASsnxu5b"
      },
      "outputs": [],
      "source": [
        "# # myfile = open('gdrive/MyDrive/2A/MODAL/data/train_cercle/attempt_4_circle_201_r_3.47_bruit_1.png')\n",
        "# # plt.imshow(myfile)\n",
        "\n",
        "# img = mpimg.imread('gdrive/MyDrive/2A/MODAL/data/train_cercle/attempt_4_circle_201_r_3.47_bruit_1.png')\n",
        "# #img = tf.keras.utils.normalize(img,axis = 1)\n",
        "# imgplot = plt.imshow(img)\n",
        "# plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9_yULZ-cbR2E"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
